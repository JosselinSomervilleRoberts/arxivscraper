{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e301dee6540c441d8d500d25af408d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Authenticate to Hugging Face in a notebook environment\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f76d5ecf5614fdea4123b20aa51af60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb64f80044b4c43a378a088815e23d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The columns in features (['id', 'tex_code', 'category', 'subject', 'output', 'asset_0', 'asset_1', 'asset_2', 'asset_3', 'asset_4', 'asset_5', 'asset_6', 'asset_7', 'asset_8', 'asset_9']) must be identical as the columns in the dataset: ['id', 'tex_code', 'category', 'subject', 'output', 'asset_0', 'asset_1', 'asset_2', 'asset_3', 'asset_4', 'asset_5', 'asset_6', 'asset_7', 'asset_8', 'asset_9', '__index_level_0__']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Define the features of your dataset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m features \u001b[38;5;241m=\u001b[39m Features({\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: Value(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtex_code\u001b[39m\u001b[38;5;124m'\u001b[39m: Value(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masset_9\u001b[39m\u001b[38;5;124m'\u001b[39m: Value(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     64\u001b[0m })\n\u001b[0;32m---> 66\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m valid_dataset\u001b[38;5;241m.\u001b[39mcast(features)\n\u001b[1;32m     69\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m DatasetDict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_dataset})\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/datasets/arrow_dataset.py:2043\u001b[0m, in \u001b[0;36mDataset.cast\u001b[0;34m(self, features, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, num_proc)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;124;03mCast the dataset to a new set of features.\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(features) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names):\n\u001b[0;32m-> 2043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe columns in features (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be identical \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2045\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas the columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2046\u001b[0m     )\n\u001b[1;32m   2048\u001b[0m schema \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39marrow_schema\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat\n",
      "\u001b[0;31mValueError\u001b[0m: The columns in features (['id', 'tex_code', 'category', 'subject', 'output', 'asset_0', 'asset_1', 'asset_2', 'asset_3', 'asset_4', 'asset_5', 'asset_6', 'asset_7', 'asset_8', 'asset_9']) must be identical as the columns in the dataset: ['id', 'tex_code', 'category', 'subject', 'output', 'asset_0', 'asset_1', 'asset_2', 'asset_3', 'asset_4', 'asset_5', 'asset_6', 'asset_7', 'asset_8', 'asset_9', '__index_level_0__']"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from toolbox.printing import debug\n",
    "import random\n",
    "CATEGORIES = [\"equation\", \"figure\", \"table\", \"plot\", \"algorithm\"]\n",
    "CAT_INDEX = 0\n",
    "\n",
    "for category in CATEGORIES[CAT_INDEX:CAT_INDEX+1]:\n",
    "    df = pd.read_csv(f\"dataset_{category}.csv\")\n",
    "    # Shuffle dataframe\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    def load_image(image_path):\n",
    "        if image_path.lower().endswith(\".png\") or image_path.lower().endswith(\".jpg\"):\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                return Image.open(io.BytesIO(f.read()))\n",
    "        else:\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                return io.BytesIO(f.read())\n",
    "\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "\n",
    "    def transform(row):\n",
    "        row[\"tex_code\"] = read_file(row[\"tex_code\"])\n",
    "        for key in [\"output\"]:\n",
    "            if row[key] is not None and len(row[key]) > 0:\n",
    "                row[key] = load_image(row[key])\n",
    "        return row\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df).map(transform).shuffle()\n",
    "    valid_dataset = Dataset.from_pandas(valid_df).map(transform).shuffle()\n",
    "\n",
    "    from datasets import Features, Value, Image, load_dataset\n",
    "\n",
    "    # Define the features of your dataset\n",
    "    features = Features({\n",
    "        'id': Value('int64'),\n",
    "        'tex_code': Value('string'),\n",
    "        'category': Value('string'),\n",
    "        'subject': Value('string'),\n",
    "        'output': Image(),        # For image features\n",
    "        'asset_0': Value('string'),\n",
    "        'asset_1': Value('string'),\n",
    "        'asset_2': Value('string'),\n",
    "        'asset_3': Value('string'),\n",
    "        'asset_4': Value('string'),\n",
    "        'asset_5': Value('string'),\n",
    "        'asset_6': Value('string'),\n",
    "        'asset_7': Value('string'),\n",
    "        'asset_8': Value('string'),\n",
    "        'asset_9': Value('string'),\n",
    "        '__index_level_0__': Value('int64')\n",
    "    })\n",
    "\n",
    "    train_dataset = train_dataset.cast(features)\n",
    "    valid_dataset = valid_dataset.cast(features)\n",
    "\n",
    "    dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": valid_dataset})\n",
    "    # help(dataset_dict.push_to_hub)\n",
    "    # Then save or upload as before\n",
    "\n",
    "\n",
    "    dataset_dict.push_to_hub(\"JosselinSom/Latex-VLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
